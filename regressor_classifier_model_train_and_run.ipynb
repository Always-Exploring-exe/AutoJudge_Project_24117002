{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daf413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler # Pure FP16 Imports\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports (notebook runs from notebooks/ directory)\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from src.data.data_loader import load_data\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\" \n",
    "MAX_LEN = 350       \n",
    "BATCH_SIZE = 8    # Kept small (4) because DeBERTa-Large is heavy even with FP16\n",
    "EPOCHS = 10\n",
    "LR = 1e-4           \n",
    "WEIGHT_DECAY = 0.1  \n",
    "LORA_DROPOUT = 0.1\n",
    "# Model will be saved to models/ directory in project root\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", \"autojudge_best_acc_model.pth\")\n",
    "PATIENCE = 3\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True) \n",
    "\n",
    "# --- MODEL DEFINITION ---\n",
    "class AutoJudge(nn.Module):\n",
    "    def __init__(self, model_name, freeze_bert=False):\n",
    "        super(AutoJudge, self).__init__()\n",
    "        \n",
    "        # Load Model (Safetensors to fix security error)\n",
    "        # Gradient Checkpointing REMOVED\n",
    "        self.bert = AutoModel.from_pretrained(model_name, use_safetensors=True)\n",
    "        \n",
    "        self.bert_dim = self.bert.config.hidden_size \n",
    "        \n",
    "        self.processor = nn.Sequential(\n",
    "            nn.Linear(self.bert_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(256, 3) \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.processor(cls_emb)\n",
    "        \n",
    "        logits_cls = self.classifier(x)\n",
    "        raw_score = self.regressor(x)\n",
    "        final_score = raw_score * 9.0 + 1.0 \n",
    "        \n",
    "        return logits_cls, final_score\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df['full_text'].tolist()\n",
    "        self.y_cls = df['label_cls'].tolist()\n",
    "        self.y_score = df['label_score'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'ids': enc['input_ids'].squeeze(0),\n",
    "            'mask': enc['attention_mask'].squeeze(0),\n",
    "            'y_cls': torch.tensor(self.y_cls[idx], dtype=torch.long),\n",
    "            'y_score': torch.tensor(self.y_score[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def evaluate(model, loader, device, criterion_cls, criterion_reg):\n",
    "    model.eval()\n",
    "    all_preds_cls = []\n",
    "    all_targets_cls = []\n",
    "    all_preds_score = []\n",
    "    all_targets_score = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use Autocast in eval too for speed/memory\n",
    "        with autocast():\n",
    "            for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "                ids = batch['ids'].to(device)\n",
    "                mask = batch['mask'].to(device)\n",
    "                y_c = batch['y_cls'].to(device)\n",
    "                y_s = batch['y_score'].to(device)\n",
    "                \n",
    "                logits, scores = model(ids, mask)\n",
    "                \n",
    "                loss = criterion_cls(logits, y_c) + criterion_reg(scores.squeeze(), y_s)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                preds_c = torch.argmax(logits, dim=1)\n",
    "                all_preds_cls.extend(preds_c.cpu().numpy())\n",
    "                all_targets_cls.extend(y_c.cpu().numpy())\n",
    "                \n",
    "                all_preds_score.extend(scores.squeeze().cpu().numpy())\n",
    "                all_targets_score.extend(y_s.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_targets_cls, all_preds_cls)\n",
    "    mae = mean_absolute_error(all_targets_score, all_preds_score)\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets_score, all_preds_score))\n",
    "    \n",
    "    return avg_loss, acc, mae, rmse\n",
    "\n",
    "def train_and_eval():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    df = load_data()\n",
    "    if df is None: return\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    print(f\"ðŸ“Š Dataset Split: {len(train_df)} Training | {len(test_df)} Testing\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸš€ Device: {device} | Model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Tokenizer fix\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    \n",
    "    train_ds = TaskDataset(train_df, tokenizer)\n",
    "    test_ds = TaskDataset(test_df, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = AutoJudge(MODEL_NAME, freeze_bert=False)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=16, \n",
    "        lora_alpha=32, \n",
    "        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"], \n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    model.bert = get_peft_model(model.bert, peft_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # FP16 Scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    loss_fn_cls = nn.CrossEntropyLoss()\n",
    "    loss_fn_reg = nn.MSELoss()\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"\\nðŸ‹ï¸ Starting Training (Pure FP16 Mode)...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for batch in loop:\n",
    "            ids = batch['ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            y_c = batch['y_cls'].to(device)\n",
    "            y_s = batch['y_score'].to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- FP16 CONTEXT ---\n",
    "            with autocast():\n",
    "                pred_c, pred_s = model(ids, mask)\n",
    "                loss = loss_fn_cls(pred_c, y_c) + loss_fn_reg(pred_s, y_s)\n",
    "            \n",
    "            # Scale Loss -> Backward -> Step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, acc, mae, rmse = evaluate(model, test_loader, device, loss_fn_cls, loss_fn_reg)\n",
    "        print(f\"   ðŸ“‰ Val Loss: {val_loss:.4f} | ðŸ† Acc: {acc*100:.2f}% | MAE: {mae:.2f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            early_stop_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_state, MODEL_PATH)\n",
    "            print(f\"   âœ… New Best Accuracy! Model saved.\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"   âš ï¸ Accuracy didn't improve ({early_stop_counter}/{PATIENCE})\")\n",
    "            \n",
    "        if early_stop_counter >= PATIENCE:\n",
    "            print(f\"\\nâ›” Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nðŸ”„ Loading model with Highest Accuracy ({best_acc*100:.2f}%)...\")\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    final_loss, final_acc, final_mae, final_rmse = evaluate(model, test_loader, device, loss_fn_cls, loss_fn_reg)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"ðŸ“¢ FINAL EVALUATION REPORT (Best Accuracy Model)\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"âœ… Classification Accuracy : {final_acc*100:.2f}%\")\n",
    "    print(f\"âœ… Regression MAE (Score)  : {final_mae:.4f}\")\n",
    "    print(f\"âœ… Regression RMSE (Score) : {final_rmse:.4f}\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
